{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAB_JennyChiou\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "from scipy.linalg import norm\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你 在 幹 嘛 呢', '你 在 幹 什 麼 呢']\n",
      "向量： \n",
      " [[0.         0.4090901  0.4090901  0.57496187 0.4090901  0.4090901\n",
      "  0.        ]\n",
      " [0.49844628 0.35464863 0.35464863 0.         0.35464863 0.35464863\n",
      "  0.49844628]]\n",
      "相似度： 0.5803329846765686\n"
     ]
    }
   ],
   "source": [
    "def tfidf_similarity(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ' '.join(list(s))\n",
    "\n",
    "    # 將字中間加入空格\n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # 轉化為TF矩陣\n",
    "    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    print(corpus)\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    print('向量：','\\n',vectors)\n",
    "    # 計算TF係數\n",
    "    return np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1]))\n",
    "\n",
    "s1 = '你在幹嘛呢'\n",
    "s2 = '你在幹什麼呢'\n",
    "print('相似度：',tfidf_similarity(s1, s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LAB_JennyChiou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LAB_JennyChiou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"I buy a new car for the bitch (for real)\\nI tear down the mall with the bitch (for real)\\nYou can|t even talk to the bitch (no)\\nShe fucking with bosses and shit (oh God)\\nI pull up in |rari|s and shit, with choppers and Harley|s and shit (for real)\\nI be Gucci|d down, you wearing Lacoste and shit (bitch)\\nYeah, Moncler, yeah, fur came off of that, yeah (yeah)\\nTriple homicide, put me in a chair, yeah (in jail)\\nTriple cross the plug, we do not play fair, yeah (Oh god)\\nGot |em tennis chains on and they real blingy (blingy)\\nDraco make you do the chicken head like Chingy (Chingy)\\nWalk in Neiman Marcus and I spend a light fifty (fifty)\\nPlease proceed with caution, shooters, they be right with me (21)\\nBad bitch, cute face and some nice titties\\n$7500 on a Saint Laurent jacket (yeah)\\nBitch, be careful where you dumpin| your ashes (bitch)\\nI ain|t no sucker, I ain|t cut for no action (nah)\\nThe skreets raised me, I|m a ho bastard (wild, wild, wild, wild)\\nI bought a |Rari just so I can go faster (skrrt)\\nNiggas tryna copy me, they playin| catch up (21)\\nI might pull up in a Ghost, no Casper (21)\\nI been smoking gas and I got no asthmaI got 1-2-3-4-5-6-7-8 M|s in my bank account, yeah (Oh, God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nI got 1-2-3-4-5-6-7-8 shooters ready to gun you down, yeah (fast)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)Yeah dog I|m for real, dog (21)\\nRegular, buy the seats, I got a house on the hill, dog (21)\\nWanna see a body, nigga? Get you killed, dog (wet)\\nWanna Tweet about me, nigga? Get you killed, dog (wet)\\nKilled dog, I|m a real dog, you a lil| dog (21)\\nBe a dog, wanna be a dog, chasing mil|s, dog\\nDunk right in your bitch like O|Neal, dog\\nI shoot like Reggie Mill|, dog (21)\\nChopper sting you like a eel, dogI got 1-2-3-4-5-6-7-8 M|s in my bank account, yeah (Oh, God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nI got 1-2-3-4-5-6-7-8 shooters ready to gun you down, yeah (fast)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)Roulette clips, send a roulette hit\\nPull up on your bitch, she say I got that ruler dick\\nSpray your block down, we not really with that rural shit\\nGlock cocked now, I don|t really give no fuck |bout who I hit\\nYeah, your bitch, she get jiggy with me\\nKeep that Siggy with me\\nBitch, I|m Mad Max, you know I got Ziggy with me\\nKeep a mad mag in case they wanna get busy with me\\n|Rari matte black and I got a Bentley with meI got 1-2-3-4-5-6-7-8 M|s in my bank account, yeah (Oh, God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nI got 1-2-3-4-5-6-7-8 shooters ready to gun you down, yeah (fast)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)$7500 on a Saint Laurent jacket (yeah)\\nBitch, be careful where you dumpin| your ashes (bitch)\\nI ain|t no sucker, I ain|t cut for no action (nah)\\nThe skreets raised me, I|m a ho bastard\"\n",
    "text2 = \"I drove by all the places we used to hang out getting wasted\\nI thought about our last kiss, how it felt the way you tasted\\nAnd even though your friends tell me you|re doing fineAre you somewhere feeling lonely even though he|s right beside you?\\nWhen he says those words that hurt you, do you read the ones I wrote you?Sometimes I start to wonder, was it just a lie?\\nIf what we had was real, how could you be fine?|Cause I|m not fine at allI remember the day you told me you were leaving\\nI remember the make-up running down your face\\nAnd the dreams you left behind you didn|t need them\\nLike every single wish we ever made\\nI wish that I could wake up with amnesia\\nAnd forget about the stupid little things\\nLike the way it felt to fall asleep next to you\\nAnd the memories I never can escape|Cause I|m not fine at allThe pictures that you sent me they|re still living in my phone\\nI|ll admit I like to see them, I|ll admit I feel alone\\nAnd all my friends keep asking why I|m not aroundIt hurts to know you|re happy, yeah, it hurts that you|ve moved on\\nIt|s hard to hear your name when I haven|t seen you in so longIt|s like we never happened, was it just a lie?\\nIf what we had was real, how could you be fine?|Cause I|m not fine at allI remember the day you told me you were leaving\\nI remember the make-up running down your face\\nAnd the dreams you left behind you didn|t need them\\nLike every single wish we ever made\\nI wish that I could wake up with amnesia\\nAnd forget about the stupid little things\\nLike the way it felt to fall asleep next to you\\nAnd the memories I never can escapeIf today I woke up with you right beside me\\nLike all of this was just some twisted dream\\nI|d hold you closer than I ever did before\\nAnd you|d never slip away\\nAnd you|d never hear me sayI remember the day you told me you were leaving\\nI remember the make-up running down your face\\nAnd the dreams you left behind you didn|t need them\\nLike every single wish we ever made\\nI wish that I could wake up with amnesia\\nAnd forget about the stupid little things\\nLike the way it felt to fall asleep next to you\\nAnd the memories I never can escape\\n|Cause I|m not fine at all\\nNo, I|m really not fine at all\\nTell me this is just a dream\\n|Cause I|m really not fine at all\"\n",
    "text3 = \"I though we had a place, just our place, our home place, my headspace\\nWas you and I always, but that phase has been phased in our place\\nI see it on your face, a small trace, a blank slate, we been erased\\nBut if we|re way too faded to drive, you can stay one more nightWe said we|d both love higher than we knew we could go\\nBut still the hardest part is knowing when to let go\\nYou wanted to go higher, higher, higher\\nBurn too bright, now the fire|s gone, watch it all fall down, BabylonBabylon\\nBurn too bright, now the fire|s gone, watch it all fall downI|m tired of the feud, your short fuse, my half turths are not amused\\nI wish we had a clue to start new, a white moon, no residue\\nThe color of our mood is so rude, a cold June, we|re not immune\\nBut if we|re way too faded to fight, you can stay one more nightWe said we|d both love higher than we knew we could go\\nBut still the hardest part is knowing when to let go\\nYou wanted to go higher, higher, higher\\nBurn too bright, now the fire|s gone, watch it all fall down, BabylonBabylon\\nBurn too bright, now the fire|s gone, watch it all fall downWe said we|d both love higher than we knew we could go\\nBut still the hardest part is knowing when to let go\\nYou wanted to go higher, higher, higher\\nBurn too bright, now the fire|s gone, watch it all fall down\\nWe said we|d both love higher than we knew we could go\\nBut still the hardest part is knowing when to let go\\nYou wanted to go higher, higher, higher\\nBurn too bright, now the fire|s gone, watch it all fall down, BabylonBabylon\\nBurn too bright, now the fire|s gone, watch it all fall down\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分詞\n",
    "def get_tokens(text):\n",
    "    lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    no_punctuation = lowers.translate(remove_punctuation_map)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#過濾缺乏實際意義的 the, a, and 等詞\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = get_tokens(text1)\n",
    "tokens2 = get_tokens(text2)\n",
    "tokens3 = get_tokens(text3)\n",
    "\n",
    "filtered1 = [w for w in tokens1 if not w in stopwords.words('english')]\n",
    "filtered2 = [w for w in tokens2 if not w in stopwords.words('english')]\n",
    "filtered3 = [w for w in tokens3 if not w in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未過濾\n",
      "count1_1： [('yeah', 45), ('oh', 35), ('god', 32), ('you', 28), ('in', 25), ('i', 24), ('down', 21), ('a', 19), ('to', 19), ('my', 18)]\n",
      "count2_1： [('you', 26), ('i', 21), ('the', 21), ('and', 13), ('to', 11), ('like', 9), ('me', 8), ('we', 7), ('it', 7), ('im', 7)]\n",
      "count3_1： [('higher', 16), ('the', 13), ('we', 12), ('go', 12), ('to', 11), ('too', 9), ('it', 8), ('you', 7), ('but', 7), ('burn', 7)]\n"
     ]
    }
   ],
   "source": [
    "print('未過濾')\n",
    "count1_1 = Counter(tokens1) #未過濾\n",
    "count2_1 = Counter(tokens2) #未過濾\n",
    "count3_1 = Counter(tokens3) #未過濾\n",
    "print ('count1_1：',count1_1.most_common(10))\n",
    "print ('count2_1：',count2_1.most_common(10))\n",
    "print ('count3_1：',count3_1.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已過濾(停用詞)\n",
      "count1_2： [('yeah', 45), ('oh', 35), ('god', 32), ('bank', 18), ('account', 18), ('ready', 18), ('gun', 18), ('bitch', 13), ('dog', 13), ('got', 12)]\n",
      "count2_2： [('like', 9), ('im', 7), ('fine', 6), ('remember', 6), ('wish', 6), ('never', 6), ('could', 5), ('felt', 4), ('way', 4), ('ever', 4)]\n",
      "count3_2： [('higher', 16), ('go', 12), ('burn', 7), ('bright', 7), ('fires', 7), ('gone', 7), ('watch', 7), ('fall', 7), ('place', 4), ('said', 4)]\n"
     ]
    }
   ],
   "source": [
    "#Counter() 函式用於統計每個單詞出現的次數。\n",
    "print('已過濾(停用詞)')\n",
    "count1_2 = Counter(filtered1) #已過濾\n",
    "count2_2 = Counter(filtered2) #已過濾\n",
    "count3_2 = Counter(filtered3) #已過濾\n",
    "print ('count1_2：',count1_2.most_common(10))\n",
    "print ('count2_2：',count2_2.most_common(10))\n",
    "print ('count3_2：',count3_2.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已過濾(保留字根)\n",
      "count1-3： [('yeah', 45), ('oh', 35), ('god', 32), ('bank', 18), ('account', 18), ('readi', 18), ('gun', 18), ('bitch', 13), ('dog', 13), ('got', 12)]\n",
      "count2-3： [('like', 9), ('im', 7), ('fine', 6), ('rememb', 6), ('wish', 6), ('never', 6), ('could', 5), ('dream', 5), ('felt', 4), ('way', 4)]\n",
      "count3-3： [('higher', 16), ('go', 12), ('burn', 7), ('bright', 7), ('fire', 7), ('gone', 7), ('watch', 7), ('fall', 7), ('place', 4), ('said', 4)]\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed1 = stem_tokens(filtered1, stemmer)\n",
    "stemmed2 = stem_tokens(filtered2, stemmer)\n",
    "stemmed3 = stem_tokens(filtered3, stemmer)\n",
    "\n",
    "print('已過濾(保留字根)')\n",
    "count1_3= Counter(stemmed1)\n",
    "print('count1-3：',count1_3.most_common(10))\n",
    "count2_3= Counter(stemmed2)\n",
    "print('count2-3：',count2_3.most_common(10))\n",
    "count3_3= Counter(stemmed3)\n",
    "print('count3-3：',count3_3.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算 TF-IDF(t)=TF(t)×IDF(t)\n",
    "#教學：https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/\n",
    "def tf(word, count):\n",
    "    return count[word] / sum(count.values())\n",
    "\n",
    "def n_containing(word, count_list):\n",
    "    return sum(1 for count in count_list if word in count)\n",
    "\n",
    "def idf(word, count_list):\n",
    "    return math.log(len(count_list) / (1+n_containing(word, count_list)))\n",
    "\n",
    "def tfidf(word, count, count_list):\n",
    "    return tf(word, count) * idf(word, count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Top words in document 1\n",
      "Word: oh, TF-IDF: 0.03112\n",
      "Word: god, TF-IDF: 0.02845\n",
      "Word: bank, TF-IDF: 0.01601\n",
      "Word: account, TF-IDF: 0.01601\n",
      "Word: ready, TF-IDF: 0.01601\n",
      "Word: gun, TF-IDF: 0.01601\n",
      "Word: bitch, TF-IDF: 0.01156\n",
      "Word: dog, TF-IDF: 0.01156\n",
      "Word: got, TF-IDF: 0.01067\n",
      "Word: 21, TF-IDF: 0.00622\n",
      "------------------------------------------\n",
      "Top words in document 2\n",
      "Word: fine, TF-IDF: 0.01072\n",
      "Word: remember, TF-IDF: 0.01072\n",
      "Word: never, TF-IDF: 0.01072\n",
      "Word: felt, TF-IDF: 0.00714\n",
      "Word: ever, TF-IDF: 0.00714\n",
      "Word: day, TF-IDF: 0.00536\n",
      "Word: told, TF-IDF: 0.00536\n",
      "Word: leaving, TF-IDF: 0.00536\n",
      "Word: makeup, TF-IDF: 0.00536\n",
      "Word: running, TF-IDF: 0.00536\n",
      "------------------------------------------\n",
      "Top words in document 3\n",
      "Word: higher, TF-IDF: 0.03862\n",
      "Word: burn, TF-IDF: 0.01689\n",
      "Word: bright, TF-IDF: 0.01689\n",
      "Word: fires, TF-IDF: 0.01689\n",
      "Word: gone, TF-IDF: 0.01689\n",
      "Word: watch, TF-IDF: 0.01689\n",
      "Word: place, TF-IDF: 0.00965\n",
      "Word: said, TF-IDF: 0.00965\n",
      "Word: wed, TF-IDF: 0.00965\n",
      "Word: love, TF-IDF: 0.00965\n"
     ]
    }
   ],
   "source": [
    "countlist = [count1_2, count2_2, count3_2]\n",
    "for i, count in enumerate(countlist):\n",
    "    print('------------------------------------------')\n",
    "    print(\"Top words in document {}\".format(i+1))\n",
    "    scores = {word: tfidf(word, count, countlist) for word in count}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:10]:\n",
    "        print(\"Word: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buy', 'new', 'car', 'bitch', 'real', 'tear', 'mall', 'bitch', 'real', 'cant', 'even', 'talk', 'bitch', 'fucking', 'bosses', 'shit', 'oh', 'god', 'pull', 'raris', 'shit', 'choppers', 'harleys', 'shit', 'real', 'guccid', 'wearing', 'lacoste', 'shit', 'bitch', 'yeah', 'moncler', 'yeah', 'fur', 'came', 'yeah', 'yeah', 'triple', 'homicide', 'put', 'chair', 'yeah', 'jail', 'triple', 'cross', 'plug', 'play', 'fair', 'yeah', 'oh', 'god', 'got', 'em', 'tennis', 'chains', 'real', 'blingy', 'blingy', 'draco', 'make', 'chicken', 'head', 'like', 'chingy', 'chingy', 'walk', 'neiman', 'marcus', 'spend', 'light', 'fifty', 'fifty', 'please', 'proceed', 'caution', 'shooters', 'right', '21', 'bad', 'bitch', 'cute', 'face', 'nice', 'titties', '7500', 'saint', 'laurent', 'jacket', 'yeah', 'bitch', 'careful', 'dumpin', 'ashes', 'bitch', 'aint', 'sucker', 'aint', 'cut', 'action', 'nah', 'skreets', 'raised', 'im', 'ho', 'bastard', 'wild', 'wild', 'wild', 'wild', 'bought', 'rari', 'go', 'faster', 'skrrt', 'niggas', 'tryna', 'copy', 'playin', 'catch', '21', 'might', 'pull', 'ghost', 'casper', '21', 'smoking', 'gas', 'got', 'asthmai', 'got', '12345678', 'ms', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'got', '12345678', 'shooters', 'ready', 'gun', 'yeah', 'fast', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'godyeah', 'dog', 'im', 'real', 'dog', '21', 'regular', 'buy', 'seats', 'got', 'house', 'hill', 'dog', '21', 'wan', 'na', 'see', 'body', 'nigga', 'get', 'killed', 'dog', 'wet', 'wan', 'na', 'tweet', 'nigga', 'get', 'killed', 'dog', 'wet', 'killed', 'dog', 'im', 'real', 'dog', 'lil', 'dog', '21', 'dog', 'wan', 'na', 'dog', 'chasing', 'mils', 'dog', 'dunk', 'right', 'bitch', 'like', 'oneal', 'dog', 'shoot', 'like', 'reggie', 'mill', 'dog', '21', 'chopper', 'sting', 'like', 'eel', 'dogi', 'got', '12345678', 'ms', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'got', '12345678', 'shooters', 'ready', 'gun', 'yeah', 'fast', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'godroulette', 'clips', 'send', 'roulette', 'hit', 'pull', 'bitch', 'say', 'got', 'ruler', 'dick', 'spray', 'block', 'really', 'rural', 'shit', 'glock', 'cocked', 'dont', 'really', 'give', 'fuck', 'bout', 'hit', 'yeah', 'bitch', 'get', 'jiggy', 'keep', 'siggy', 'bitch', 'im', 'mad', 'max', 'know', 'got', 'ziggy', 'keep', 'mad', 'mag', 'case', 'wan', 'na', 'get', 'busy', 'rari', 'matte', 'black', 'got', 'bentley', 'mei', 'got', '12345678', 'ms', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'bank', 'account', 'yeah', 'oh', 'god', 'got', '12345678', 'shooters', 'ready', 'gun', 'yeah', 'fast', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god', 'ready', 'gun', 'yeah', 'oh', 'god7500', 'saint', 'laurent', 'jacket', 'yeah', 'bitch', 'careful', 'dumpin', 'ashes', 'bitch', 'aint', 'sucker', 'aint', 'cut', 'action', 'nah', 'skreets', 'raised', 'im', 'ho', 'bastard']\n"
     ]
    }
   ],
   "source": [
    "text = \"I buy a new car for the bitch (for real)\\nI tear down the mall with the bitch (for real)\\nYou can|t even talk to the bitch (no)\\nShe fucking with bosses and shit (oh God)\\nI pull up in |rari|s and shit, with choppers and Harley|s and shit (for real)\\nI be Gucci|d down, you wearing Lacoste and shit (bitch)\\nYeah, Moncler, yeah, fur came off of that, yeah (yeah)\\nTriple homicide, put me in a chair, yeah (in jail)\\nTriple cross the plug, we do not play fair, yeah (Oh god)\\nGot |em tennis chains on and they real blingy (blingy)\\nDraco make you do the chicken head like Chingy (Chingy)\\nWalk in Neiman Marcus and I spend a light fifty (fifty)\\nPlease proceed with caution, shooters, they be right with me (21)\\nBad bitch, cute face and some nice titties\\n$7500 on a Saint Laurent jacket (yeah)\\nBitch, be careful where you dumpin| your ashes (bitch)\\nI ain|t no sucker, I ain|t cut for no action (nah)\\nThe skreets raised me, I|m a ho bastard (wild, wild, wild, wild)\\nI bought a |Rari just so I can go faster (skrrt)\\nNiggas tryna copy me, they playin| catch up (21)\\nI might pull up in a Ghost, no Casper (21)\\nI been smoking gas and I got no asthmaI got 1-2-3-4-5-6-7-8 M|s in my bank account, yeah (Oh, God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nI got 1-2-3-4-5-6-7-8 shooters ready to gun you down, yeah (fast)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)Yeah dog I|m for real, dog (21)\\nRegular, buy the seats, I got a house on the hill, dog (21)\\nWanna see a body, nigga? Get you killed, dog (wet)\\nWanna Tweet about me, nigga? Get you killed, dog (wet)\\nKilled dog, I|m a real dog, you a lil| dog (21)\\nBe a dog, wanna be a dog, chasing mil|s, dog\\nDunk right in your bitch like O|Neal, dog\\nI shoot like Reggie Mill|, dog (21)\\nChopper sting you like a eel, dogI got 1-2-3-4-5-6-7-8 M|s in my bank account, yeah (Oh, God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nI got 1-2-3-4-5-6-7-8 shooters ready to gun you down, yeah (fast)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)Roulette clips, send a roulette hit\\nPull up on your bitch, she say I got that ruler dick\\nSpray your block down, we not really with that rural shit\\nGlock cocked now, I don|t really give no fuck |bout who I hit\\nYeah, your bitch, she get jiggy with me\\nKeep that Siggy with me\\nBitch, I|m Mad Max, you know I got Ziggy with me\\nKeep a mad mag in case they wanna get busy with me\\n|Rari matte black and I got a Bentley with meI got 1-2-3-4-5-6-7-8 M|s in my bank account, yeah (Oh, God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nIn my bank account, yeah (Oh God)\\nI got 1-2-3-4-5-6-7-8 shooters ready to gun you down, yeah (fast)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)\\nReady to gun you down, yeah (Oh God)$7500 on a Saint Laurent jacket (yeah)\\nBitch, be careful where you dumpin| your ashes (bitch)\\nI ain|t no sucker, I ain|t cut for no action (nah)\\nThe skreets raised me, I|m a ho bastard\"\n",
    "tokens1 = get_tokens(text)\n",
    "filtered1 = [w for w in tokens1 if not w in stopwords.words('english')]\n",
    "print(filtered1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer是透過fit_transform函數將文本中的詞語轉換為詞頻矩陣\n",
    "#矩陣元素a[i][j] 表示j詞在第i個文本下的詞頻，即各个詞語出現的次數。\n",
    "#透過get_feature_names()可看到所有文本的關鍵字，透過toarray()可看到詞頻矩陣的結果。\n",
    "cv = CountVectorizer(max_df=0.85)\n",
    "word_count_vector =cv.fit_transform(filtered1)\n",
    "word_count_array =word_count_vector.toarray()\n",
    "word_count_names =cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456, 164)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12345678',\n",
       " '21',\n",
       " '7500',\n",
       " 'account',\n",
       " 'action',\n",
       " 'aint',\n",
       " 'ashes',\n",
       " 'asthmai',\n",
       " 'bad',\n",
       " 'bank',\n",
       " 'bastard',\n",
       " 'bentley',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'blingy',\n",
       " 'block',\n",
       " 'body',\n",
       " 'bosses',\n",
       " 'bought',\n",
       " 'bout',\n",
       " 'busy',\n",
       " 'buy',\n",
       " 'came',\n",
       " 'cant',\n",
       " 'car',\n",
       " 'careful',\n",
       " 'case',\n",
       " 'casper',\n",
       " 'catch',\n",
       " 'caution',\n",
       " 'chains',\n",
       " 'chair',\n",
       " 'chasing',\n",
       " 'chicken',\n",
       " 'chingy',\n",
       " 'chopper',\n",
       " 'choppers',\n",
       " 'clips',\n",
       " 'cocked',\n",
       " 'copy',\n",
       " 'cross',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'dick',\n",
       " 'dog',\n",
       " 'dogi',\n",
       " 'dont',\n",
       " 'draco',\n",
       " 'dumpin',\n",
       " 'dunk',\n",
       " 'eel',\n",
       " 'em',\n",
       " 'even',\n",
       " 'face',\n",
       " 'fair',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'fifty',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'fur',\n",
       " 'gas',\n",
       " 'get',\n",
       " 'ghost',\n",
       " 'give',\n",
       " 'glock',\n",
       " 'go',\n",
       " 'god',\n",
       " 'god7500',\n",
       " 'godroulette',\n",
       " 'godyeah',\n",
       " 'got',\n",
       " 'guccid',\n",
       " 'gun',\n",
       " 'harleys',\n",
       " 'head',\n",
       " 'hill',\n",
       " 'hit',\n",
       " 'ho',\n",
       " 'homicide',\n",
       " 'house',\n",
       " 'im',\n",
       " 'jacket',\n",
       " 'jail',\n",
       " 'jiggy',\n",
       " 'keep',\n",
       " 'killed',\n",
       " 'know',\n",
       " 'lacoste',\n",
       " 'laurent',\n",
       " 'light',\n",
       " 'like',\n",
       " 'lil',\n",
       " 'mad',\n",
       " 'mag',\n",
       " 'make',\n",
       " 'mall',\n",
       " 'marcus',\n",
       " 'matte',\n",
       " 'max',\n",
       " 'mei',\n",
       " 'might',\n",
       " 'mill',\n",
       " 'mils',\n",
       " 'moncler',\n",
       " 'ms',\n",
       " 'na',\n",
       " 'nah',\n",
       " 'neiman',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'nigga',\n",
       " 'niggas',\n",
       " 'oh',\n",
       " 'oneal',\n",
       " 'play',\n",
       " 'playin',\n",
       " 'please',\n",
       " 'plug',\n",
       " 'proceed',\n",
       " 'pull',\n",
       " 'put',\n",
       " 'raised',\n",
       " 'rari',\n",
       " 'raris',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reggie',\n",
       " 'regular',\n",
       " 'right',\n",
       " 'roulette',\n",
       " 'ruler',\n",
       " 'rural',\n",
       " 'saint',\n",
       " 'say',\n",
       " 'seats',\n",
       " 'see',\n",
       " 'send',\n",
       " 'shit',\n",
       " 'shoot',\n",
       " 'shooters',\n",
       " 'siggy',\n",
       " 'skreets',\n",
       " 'skrrt',\n",
       " 'smoking',\n",
       " 'spend',\n",
       " 'spray',\n",
       " 'sting',\n",
       " 'sucker',\n",
       " 'talk',\n",
       " 'tear',\n",
       " 'tennis',\n",
       " 'titties',\n",
       " 'triple',\n",
       " 'tryna',\n",
       " 'tweet',\n",
       " 'walk',\n",
       " 'wan',\n",
       " 'wearing',\n",
       " 'wet',\n",
       " 'wild',\n",
       " 'yeah',\n",
       " 'ziggy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buy',\n",
       " 'new',\n",
       " 'car',\n",
       " 'bitch',\n",
       " 'real',\n",
       " 'tear',\n",
       " 'mall',\n",
       " 'cant',\n",
       " 'even',\n",
       " 'talk',\n",
       " 'fucking',\n",
       " 'bosses',\n",
       " 'shit',\n",
       " 'oh',\n",
       " 'god',\n",
       " 'pull',\n",
       " 'raris',\n",
       " 'choppers',\n",
       " 'harleys',\n",
       " 'guccid',\n",
       " 'wearing',\n",
       " 'lacoste',\n",
       " 'yeah',\n",
       " 'moncler',\n",
       " 'fur',\n",
       " 'came',\n",
       " 'triple',\n",
       " 'homicide',\n",
       " 'put',\n",
       " 'chair']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TfidfTransformer統計vectorizer中每個詞語的tf-idf權重值\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.17877324, 5.04524185, 6.43153621, 4.18024441, 6.0260711 ,\n",
       "       5.51524548, 6.0260711 , 6.43153621, 6.43153621, 4.18024441,\n",
       "       6.0260711 , 6.43153621, 4.48562606, 6.43153621, 6.0260711 ,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 6.43153621,\n",
       "       6.43153621, 6.0260711 , 6.43153621, 6.43153621, 6.43153621,\n",
       "       6.0260711 , 6.43153621, 6.43153621, 6.43153621, 6.43153621,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 6.0260711 ,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 6.43153621,\n",
       "       6.43153621, 6.0260711 , 6.43153621, 6.43153621, 4.48562606,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.0260711 , 6.43153621,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 6.43153621,\n",
       "       5.73838903, 6.43153621, 6.0260711 , 6.43153621, 6.43153621,\n",
       "       6.43153621, 6.43153621, 5.51524548, 6.43153621, 6.43153621,\n",
       "       6.43153621, 6.43153621, 3.62817583, 6.43153621, 6.43153621,\n",
       "       6.43153621, 4.55973403, 6.43153621, 4.18024441, 6.43153621,\n",
       "       6.43153621, 6.43153621, 6.0260711 , 6.0260711 , 6.43153621,\n",
       "       6.43153621, 5.33292392, 6.0260711 , 6.43153621, 6.43153621,\n",
       "       6.0260711 , 5.73838903, 6.43153621, 6.43153621, 6.0260711 ,\n",
       "       6.43153621, 5.51524548, 6.43153621, 6.0260711 , 6.43153621,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 6.43153621,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 6.43153621,\n",
       "       5.73838903, 5.51524548, 6.0260711 , 6.43153621, 6.43153621,\n",
       "       6.43153621, 6.0260711 , 6.43153621, 3.54116445, 6.43153621,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 6.43153621,\n",
       "       5.73838903, 6.43153621, 6.0260711 , 6.0260711 , 6.43153621,\n",
       "       4.18024441, 5.17877324, 6.0260711 , 6.43153621, 6.43153621,\n",
       "       6.0260711 , 6.43153621, 6.43153621, 6.43153621, 6.0260711 ,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 5.33292392,\n",
       "       6.43153621, 5.51524548, 6.43153621, 6.0260711 , 6.43153621,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 6.0260711 ,\n",
       "       6.43153621, 6.43153621, 6.43153621, 6.43153621, 6.0260711 ,\n",
       "       6.43153621, 6.43153621, 6.43153621, 5.51524548, 6.43153621,\n",
       "       6.0260711 , 5.51524548, 3.29604199, 6.43153621])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 6 fields in line 16, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-0aa2bb3854d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlyrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/LAB_JennyChiou/Anaconda3/Scripts/7 實驗進度/musicPlatform/m.platform/other/CSVTables/tracks_lyrics.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nrows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 6 fields in line 16, saw 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "lyrics = pd.read_csv(\"tracks_lyrics_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(filtered1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(456, 164)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim = cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0], dtype=int64), array([  0, 200], dtype=int64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(cos_sim>0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
